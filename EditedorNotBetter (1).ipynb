{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "spliced_path = kagglehub.dataset_download(\"erentahir/dis25k\")\n",
        "ai_path = path = kagglehub.dataset_download(\"gpch2159/ai-vs-human-syn-imgs-v2-partial\")\n",
        "path = kagglehub.dataset_download(\"tristanzhang32/ai-generated-images-vs-real-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", spliced_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbddjaox6WJR",
        "outputId": "744b8e04-5b57-4346-ced4-02538e9a8919"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/erentahir/dis25k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.39G/1.39G [00:17<00:00, 83.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/gpch2159/ai-vs-human-syn-imgs-v2-partial?dataset_version_number=9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.41G/1.41G [00:12<00:00, 122MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tristanzhang32/ai-generated-images-vs-real-images?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48.4G/48.4G [12:00<00:00, 72.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "5aa23f8b",
        "outputId": "2bc9a174-1cb7-4607-dced-f3eaf2e410d7"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "images_dir = os.path.join(spliced_path, 'DIS25k', 'images')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Load 6000 files (4000 Train + 1000 Val + 1000 Test)\n",
        "    subset_files = all_files[:6000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_spliced_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_spliced_images)}\")\n",
        "    display(df_spliced_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 24964\n",
            "Images loaded into DataFrame: 6000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "1  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "2  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "3  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "4  /root/.cache/kagglehub/datasets/erentahir/dis2..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8fda2b86-9792-467d-8a3f-9e891c5f8da4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fda2b86-9792-467d-8a3f-9e891c5f8da4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8fda2b86-9792-467d-8a3f-9e891c5f8da4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8fda2b86-9792-467d-8a3f-9e891c5f8da4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/1.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/1000.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/10.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = os.path.join(ai_path, 'stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Load 6000 files (4000 Train + 1000 Val + 1000 Test)\n",
        "    subset_files = all_files[:6000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_ai_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_ai_images)}\")\n",
        "    display(df_ai_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NBa3SZ-pM4oc",
        "outputId": "0140b667-4ad0-4887-da2b-d6f86ecc7159"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 30440\n",
            "Images loaded into DataFrame: 6000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "1  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "2  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "3  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "4  /root/.cache/kagglehub/datasets/gpch2159/ai-vs..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-871bb443-3da4-42f7-85ad-14537bb2bdbd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-871bb443-3da4-42f7-85ad-14537bb2bdbd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-871bb443-3da4-42f7-85ad-14537bb2bdbd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-871bb443-3da4-42f7-85ad-14537bb2bdbd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_0003538364d44952924d83980771e5b7.jpg\",\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_0008e244a38541bd883b29f1f17e228e.jpg\",\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_00040d088f054d379b1aae48e9f425d2.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = os.path.join(path, 'train', 'real')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Filter out known corrupt files\n",
        "    all_files = [f for f in all_files if f != '0038.jpg']\n",
        "\n",
        "    # Load 6000 files (4000 Train + 1000 Val + 1000 Test)\n",
        "    subset_files = all_files[:6000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_real_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_real_images)}\")\n",
        "    display(df_real_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "qyNCqcqFO6hr",
        "outputId": "624ec5b3-0c56-448f-c117-3db8c91ea49d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 23999\n",
            "Images loaded into DataFrame: 6000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "1  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "2  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "3  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "4  /root/.cache/kagglehub/datasets/tristanzhang32..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b4122ad-53f5-4412-ba41-251265beddab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b4122ad-53f5-4412-ba41-251265beddab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b4122ad-53f5-4412-ba41-251265beddab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b4122ad-53f5-4412-ba41-251265beddab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0002.jpg\",\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0005.jpg\",\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0003.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ba9770",
        "outputId": "f1a292a3-5d09-40f4-9f4a-36434fcb5e30"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df):\n",
        "    # First split: Train (4000) and Temp (2000)\n",
        "    train, temp = train_test_split(df, train_size=4000, test_size=2000, random_state=42)\n",
        "    # Second split: Temp (2000) into Val (1000) and Test (1000)\n",
        "    val, test = train_test_split(temp, train_size=1000, test_size=1000, random_state=42)\n",
        "    return train, val, test\n",
        "\n",
        "# Split Spliced Images\n",
        "train_spliced, val_spliced, test_spliced = split_dataset(df_spliced_images)\n",
        "\n",
        "# Split AI Images\n",
        "train_ai, val_ai, test_ai = split_dataset(df_ai_images)\n",
        "\n",
        "# Split Real Images\n",
        "train_real, val_real, test_real = split_dataset(df_real_images)\n",
        "\n",
        "# Verify the splits\n",
        "print(f\"Spliced Train: {len(train_spliced)}, Val: {len(val_spliced)}, Test: {len(test_spliced)}\")\n",
        "print(f\"AI Train: {len(train_ai)}, Val: {len(val_ai)}, Test: {len(test_ai)}\")\n",
        "print(f\"Real Train: {len(train_real)}, Val: {len(val_real)}, Test: {len(test_real)}\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliced Train: 4000, Val: 1000, Test: 1000\n",
            "AI Train: 4000, Val: 1000, Test: 1000\n",
            "Real Train: 4000, Val: 1000, Test: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "1ca88631",
        "outputId": "b6998921-1000-4f30-8543-629f53c3778d"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Add labels to the dataframes (using .copy() to avoid SettingWithCopyWarning)\n",
        "train_spliced = train_spliced.copy()\n",
        "val_spliced = val_spliced.copy()\n",
        "test_spliced = test_spliced.copy()\n",
        "\n",
        "train_spliced['label'] = 'spliced'\n",
        "val_spliced['label'] = 'spliced'\n",
        "test_spliced['label'] = 'spliced'\n",
        "\n",
        "train_ai = train_ai.copy()\n",
        "val_ai = val_ai.copy()\n",
        "test_ai = test_ai.copy()\n",
        "\n",
        "train_ai['label'] = 'ai'\n",
        "val_ai['label'] = 'ai'\n",
        "test_ai['label'] = 'ai'\n",
        "\n",
        "train_real = train_real.copy()\n",
        "val_real = val_real.copy()\n",
        "test_real = test_real.copy()\n",
        "\n",
        "train_real['label'] = 'real'\n",
        "val_real['label'] = 'real'\n",
        "test_real['label'] = 'real'\n",
        "\n",
        "# Concatenate and shuffle\n",
        "train_df = pd.concat([train_spliced, train_ai, train_real], ignore_index=True)\n",
        "train_df = shuffle(train_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "val_df = pd.concat([val_spliced, val_ai, val_real], ignore_index=True)\n",
        "val_df = shuffle(val_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "test_df = pd.concat([test_spliced, test_ai, test_real], ignore_index=True)\n",
        "test_df = shuffle(test_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Verify\n",
        "print(f\"Combined Train Set: {len(train_df)} images\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"-\" * 20)\n",
        "print(f\"Combined Validation Set: {len(val_df)} images\")\n",
        "print(val_df['label'].value_counts())\n",
        "print(\"-\" * 20)\n",
        "print(f\"Combined Test Set: {len(test_df)} images\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "display(train_df.head())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Train Set: 12000 images\n",
            "label\n",
            "spliced    4000\n",
            "ai         4000\n",
            "real       4000\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Combined Validation Set: 3000 images\n",
            "label\n",
            "ai         1000\n",
            "spliced    1000\n",
            "real       1000\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Combined Test Set: 3000 images\n",
            "label\n",
            "ai         1000\n",
            "spliced    1000\n",
            "real       1000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path    label\n",
              "0  /root/.cache/kagglehub/datasets/erentahir/dis2...  spliced\n",
              "1  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...       ai\n",
              "2  /root/.cache/kagglehub/datasets/erentahir/dis2...  spliced\n",
              "3  /root/.cache/kagglehub/datasets/tristanzhang32...     real\n",
              "4  /root/.cache/kagglehub/datasets/erentahir/dis2...  spliced"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bacc3c0-0f96-46bb-95a4-ffeb7aadcdca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "      <td>spliced</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "      <td>ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "      <td>spliced</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "      <td>spliced</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bacc3c0-0f96-46bb-95a4-ffeb7aadcdca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bacc3c0-0f96-46bb-95a4-ffeb7aadcdca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bacc3c0-0f96-46bb-95a4-ffeb7aadcdca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(train_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_1119eabb3b304155929a7e16e548b289.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/13067.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/11414.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"spliced\",\n          \"ai\",\n          \"real\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Training transform: minimal transformations (Resize + Normalize only)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transform: no augmentation, just resize + normalize\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "DlqIG-BtPWAf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        # Define label mapping\n",
        "        self.label_map = {'real': 0, 'ai': 1, 'spliced': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use iloc to access the row by integer index\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        label_str = row['label']\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image at {image_path}: {e}\")\n",
        "            # Return a black image or handle appropriately.\n",
        "            # Here we'll generate a dummy image to prevent crashing,\n",
        "            # but in production you might want to skip or raise.\n",
        "            image = Image.new('RGB', (224, 224))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.label_map.get(label_str, -1)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "jaoPzzhIUSkA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
        "val_dataset = ImageDataset(val_df, transform=val_transform)\n",
        "test_dataset = ImageDataset(test_df, transform=val_transform)"
      ],
      "metadata": {
        "id": "-yzvVLb8UkxL"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA for acceleration\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS for acceleration\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "#num_classes = len(label_encoder.classes_)\n",
        "\n",
        "resnet50.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(resnet50.fc.in_features, 3)\n",
        ")\n",
        "\n",
        "resnet50 = resnet50.to(device)\n",
        "\n",
        "# Weighted Loss as requested\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.5, 0.5, 1.5]).to(device))\n",
        "\n",
        "# Differential Learning Rates\n",
        "optimizer = optim.Adam([\n",
        "    {'params': resnet50.layer4.parameters(), 'lr': 1e-5},\n",
        "    {'params': resnet50.fc.parameters(), 'lr': 1e-3}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Model is ready to train!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlmmreS8VpG6",
        "outputId": "86698558-1e1a-4411-be4d-2714b46b2a14"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA for acceleration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is ready to train!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "num_epochs = 3\n",
        "best_val_loss = float('inf')\n",
        "patience = 7\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase with Progress Bar\n",
        "    resnet50.train()\n",
        "    train_running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for inputs, labels in train_progress_bar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with autocast():\n",
        "            outputs = resnet50(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Mixed precision backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Metrics\n",
        "        train_running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
        "\n",
        "    train_loss = train_running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation Phase with Progress Bar\n",
        "    resnet50.eval()\n",
        "    val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = resnet50(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * val_correct / val_total)\n",
        "\n",
        "    val_loss = val_running_loss / len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model based on validation loss + early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(resnet50.state_dict(), \"best_resnet50_model.pth\")\n",
        "        print(f\"Saved best model! (val_loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
        "            break\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcVXj1178pBC",
        "outputId": "c65d7edb-3b4f-4182-9525-54547b6c2b51"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1840623761.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/750 [00:00<?, ?it/s]/tmp/ipython-input-1840623761.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:   5%|▍         | 35/750 [00:36<16:55,  1.42s/it, accuracy=60, loss=0.352]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (99991727 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  45%|████▌     | 338/750 [06:16<05:58,  1.15it/s, accuracy=80.6, loss=0.181]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (98058240 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  60%|█████▉    | 448/750 [08:11<03:40,  1.37it/s, accuracy=82.7, loss=0.0479]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (161087488 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  83%|████████▎ | 622/750 [11:38<02:51,  1.34s/it, accuracy=84.7, loss=0.101]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image at /root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/12094.jpg: image file is truncated (6 bytes not processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  85%|████████▍ | 634/750 [11:51<01:36,  1.20it/s, accuracy=84.9, loss=0.0274]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (98806617 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2791 | Train Accuracy: 85.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/188 [00:00<?, ?it/s]/tmp/ipython-input-1840623761.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0913 | Validation Accuracy: 95.13%\n",
            "Saved best model! (val_loss: 0.0913)\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   5%|▌         | 41/750 [00:49<12:33,  1.06s/it, accuracy=95.3, loss=0.0121]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image at /root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/12094.jpg: image file is truncated (6 bytes not processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1150 | Train Accuracy: 94.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0789 | Validation Accuracy: 96.00%\n",
            "Saved best model! (val_loss: 0.0789)\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  21%|██▏       | 161/750 [02:59<12:08,  1.24s/it, accuracy=96.4, loss=0.0231]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image at /root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/12094.jpg: image file is truncated (6 bytes not processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0725 | Train Accuracy: 96.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.0757 | Validation Accuracy: 95.93%\n",
            "Saved best model! (val_loss: 0.0757)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def test_model(model, test_loader, device, model_path):\n",
        "\n",
        "    print(\"Loading the best model for testing...\")\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_running_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_progress_bar:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Collect predictions and labels for confusion matrix\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            test_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * test_correct / test_total)\n",
        "\n",
        "    test_loss = test_running_loss / len(test_loader)\n",
        "    test_acc = 100 * test_correct / test_total\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Generate Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['real', 'ai', 'spliced'],\n",
        "                yticklabels=['real', 'ai', 'spliced'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "test_loss, test_acc = test_model(resnet50, test_loader, device, \"best_resnet50_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "TIUGP-1J-eIU",
        "outputId": "41c2d7ff-3eb4-4cf7-a1cb-40f3920fb72f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the best model for testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  46%|████▋     | 87/188 [01:41<02:32,  1.51s/it, accuracy=96.3, loss=0.0038]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image at /root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/13021.jpg: image file is truncated (0 bytes not processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing:  62%|██████▏   | 117/188 [02:15<01:14,  1.05s/it, accuracy=96.3, loss=0.122]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (90671520 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.1034 | Test Accuracy: 96.63%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d7fec5",
        "outputId": "59a4f437-bee4-4c15-ce89-c8eee86d5a26"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# Re-instantiate the model structure\n",
        "resnet50_inference = models.resnet50(pretrained=False)\n",
        "resnet50_inference.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(resnet50_inference.fc.in_features, 3)\n",
        ")\n",
        "\n",
        "# Load the saved state dictionary\n",
        "resnet50_inference.load_state_dict(torch.load(\"best_resnet50_model.pth\"))\n",
        "resnet50_inference.eval() # Set model to evaluation mode\n",
        "\n",
        "# Move model to the correct device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "resnet50_inference = resnet50_inference.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully on {device}!\")\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4eda98",
        "outputId": "ff7aee92-b8a0-48fb-b11d-bede56560df0"
      },
      "source": [
        "# Define the transformations for a single image (similar to ImageTransform, but without augmentation)\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/Screenshot_20260214-144506_1.png\"\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Apply transformations\n",
        "input_tensor = inference_transform(image)\n",
        "input_batch = input_tensor.unsqueeze(0) # Create a mini-batch as expected by the model\n",
        "\n",
        "# Move to the same device as the model\n",
        "input_batch = input_batch.to(device)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    output = resnet50_inference(input_batch)\n",
        "\n",
        "# Get the predicted class\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "predicted_class_idx = torch.argmax(probabilities).item()\n",
        "\n",
        "# Reverse the label mapping to get the class name\n",
        "label_map = {'real': 0, 'ai': 1, 'spliced': 2}\n",
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "predicted_label = reverse_label_map[predicted_class_idx]\n",
        "\n",
        "print(f\"Predicted class for {image_path}: {predicted_label} (Probability: {probabilities[predicted_class_idx].item():.4f})\")\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for /content/Screenshot_20260214-144506_1.png: ai (Probability: 0.8565)\n"
          ]
        }
      ]
    }
  ]
}