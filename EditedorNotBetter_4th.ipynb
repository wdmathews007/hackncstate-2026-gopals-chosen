{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "spliced_path = kagglehub.dataset_download(\"erentahir/dis25k\")\n",
        "ai_path = path = kagglehub.dataset_download(\"gpch2159/ai-vs-human-syn-imgs-v2-partial\")\n",
        "path = kagglehub.dataset_download(\"tristanzhang32/ai-generated-images-vs-real-images\")\n",
        "\n",
        "print(\"Path to dataset files:\", spliced_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbddjaox6WJR",
        "outputId": "744b8e04-5b57-4346-ced4-02538e9a8919"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/erentahir/dis25k?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.39G/1.39G [00:17<00:00, 83.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/gpch2159/ai-vs-human-syn-imgs-v2-partial?dataset_version_number=9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.41G/1.41G [00:12<00:00, 122MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/tristanzhang32/ai-generated-images-vs-real-images?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48.4G/48.4G [12:00<00:00, 72.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "5aa23f8b",
        "outputId": "a4389527-0cfc-48a2-aed0-712d3efd4d0d"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "images_dir = os.path.join(spliced_path, 'DIS25k', 'images')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Load 3000 files (2000 Train + 500 Val + 500 Test)\n",
        "    subset_files = all_files[:3000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_spliced_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_spliced_images)}\")\n",
        "    display(df_spliced_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 24964\n",
            "Images loaded into DataFrame: 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "1  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "2  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "3  /root/.cache/kagglehub/datasets/erentahir/dis2...\n",
              "4  /root/.cache/kagglehub/datasets/erentahir/dis2..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccb08865-9fb0-4503-9c7a-a588f2175469\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccb08865-9fb0-4503-9c7a-a588f2175469')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ccb08865-9fb0-4503-9c7a-a588f2175469 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ccb08865-9fb0-4503-9c7a-a588f2175469');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/1.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/1000.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/10.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = os.path.join(ai_path, 'stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Load 3000 files (2000 Train + 500 Val + 500 Test)\n",
        "    subset_files = all_files[:3000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_ai_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_ai_images)}\")\n",
        "    display(df_ai_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NBa3SZ-pM4oc",
        "outputId": "125d6aa2-c1d6-4d48-af81-cc06a11233ca"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 30440\n",
            "Images loaded into DataFrame: 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "1  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "2  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "3  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...\n",
              "4  /root/.cache/kagglehub/datasets/gpch2159/ai-vs..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c65759a-e7a6-47b6-8358-682968e5e816\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c65759a-e7a6-47b6-8358-682968e5e816')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c65759a-e7a6-47b6-8358-682968e5e816 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c65759a-e7a6-47b6-8358-682968e5e816');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_0003538364d44952924d83980771e5b7.jpg\",\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_0008e244a38541bd883b29f1f17e228e.jpg\",\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_00040d088f054d379b1aae48e9f425d2.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = os.path.join(path, 'train', 'real')\n",
        "\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Filter out known corrupt files\n",
        "    all_files = [f for f in all_files if f != '0038.jpg']\n",
        "\n",
        "    # Load 3000 files (2000 Train + 500 Val + 500 Test)\n",
        "    subset_files = all_files[:3000]\n",
        "\n",
        "    file_paths = [os.path.join(images_dir, f) for f in subset_files]\n",
        "\n",
        "    df_real_images = pd.DataFrame({'image_path': file_paths})\n",
        "\n",
        "    print(f\"Total images found in directory: {len(all_files)}\")\n",
        "    print(f\"Images loaded into DataFrame: {len(df_real_images)}\")\n",
        "    display(df_real_images.head())\n",
        "\n",
        "else:\n",
        "    print(f\"Directory not found: {images_dir}\")\n",
        "\n",
        "    if os.path.exists(spliced_path):\n",
        "        print(f\"Contents of {spliced_path}:\")\n",
        "        print(os.listdir(spliced_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "qyNCqcqFO6hr",
        "outputId": "77c2977d-b474-46e0-9187-16a69c9ca49f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found in directory: 23999\n",
            "Images loaded into DataFrame: 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path\n",
              "0  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "1  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "2  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "3  /root/.cache/kagglehub/datasets/tristanzhang32...\n",
              "4  /root/.cache/kagglehub/datasets/tristanzhang32..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99694bd6-018f-4e1f-8753-4d24387b0255\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99694bd6-018f-4e1f-8753-4d24387b0255')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99694bd6-018f-4e1f-8753-4d24387b0255 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99694bd6-018f-4e1f-8753-4d24387b0255');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        print(os\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0002.jpg\",\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0005.jpg\",\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0003.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ba9770",
        "outputId": "20233d3d-ac84-4f74-8723-f6d356fb39ec"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df):\n",
        "    # First split: Train (2000) and Temp (1000)\n",
        "    train, temp = train_test_split(df, train_size=2000, test_size=1000, random_state=42)\n",
        "    # Second split: Temp (1000) into Val (500) and Test (500)\n",
        "    val, test = train_test_split(temp, train_size=500, test_size=500, random_state=42)\n",
        "    return train, val, test\n",
        "\n",
        "# Split Spliced Images\n",
        "train_spliced, val_spliced, test_spliced = split_dataset(df_spliced_images)\n",
        "\n",
        "# Split AI Images\n",
        "train_ai, val_ai, test_ai = split_dataset(df_ai_images)\n",
        "\n",
        "# Split Real Images\n",
        "train_real, val_real, test_real = split_dataset(df_real_images)\n",
        "\n",
        "# Verify the splits\n",
        "print(f\"Spliced Train: {len(train_spliced)}, Val: {len(val_spliced)}, Test: {len(test_spliced)}\")\n",
        "print(f\"AI Train: {len(train_ai)}, Val: {len(val_ai)}, Test: {len(test_ai)}\")\n",
        "print(f\"Real Train: {len(train_real)}, Val: {len(val_real)}, Test: {len(test_real)}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spliced Train: 2000, Val: 500, Test: 500\n",
            "AI Train: 2000, Val: 500, Test: 500\n",
            "Real Train: 2000, Val: 500, Test: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "1ca88631",
        "outputId": "cac12a0e-69ef-494f-877a-94483493124d"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Add labels to the dataframes (using .copy() to avoid SettingWithCopyWarning)\n",
        "train_spliced = train_spliced.copy()\n",
        "val_spliced = val_spliced.copy()\n",
        "test_spliced = test_spliced.copy()\n",
        "\n",
        "train_spliced['label'] = 'spliced'\n",
        "val_spliced['label'] = 'spliced'\n",
        "test_spliced['label'] = 'spliced'\n",
        "\n",
        "train_ai = train_ai.copy()\n",
        "val_ai = val_ai.copy()\n",
        "test_ai = test_ai.copy()\n",
        "\n",
        "train_ai['label'] = 'ai'\n",
        "val_ai['label'] = 'ai'\n",
        "test_ai['label'] = 'ai'\n",
        "\n",
        "train_real = train_real.copy()\n",
        "val_real = val_real.copy()\n",
        "test_real = test_real.copy()\n",
        "\n",
        "train_real['label'] = 'real'\n",
        "val_real['label'] = 'real'\n",
        "test_real['label'] = 'real'\n",
        "\n",
        "# Concatenate and shuffle\n",
        "train_df = pd.concat([train_spliced, train_ai, train_real], ignore_index=True)\n",
        "train_df = shuffle(train_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "val_df = pd.concat([val_spliced, val_ai, val_real], ignore_index=True)\n",
        "val_df = shuffle(val_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "test_df = pd.concat([test_spliced, test_ai, test_real], ignore_index=True)\n",
        "test_df = shuffle(test_df, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Verify\n",
        "print(f\"Combined Train Set: {len(train_df)} images\")\n",
        "print(train_df['label'].value_counts())\n",
        "print(\"-\" * 20)\n",
        "print(f\"Combined Validation Set: {len(val_df)} images\")\n",
        "print(val_df['label'].value_counts())\n",
        "print(\"-\" * 20)\n",
        "print(f\"Combined Test Set: {len(test_df)} images\")\n",
        "print(test_df['label'].value_counts())\n",
        "\n",
        "display(train_df.head())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Train Set: 6000 images\n",
            "label\n",
            "spliced    2000\n",
            "ai         2000\n",
            "real       2000\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Combined Validation Set: 1500 images\n",
            "label\n",
            "real       500\n",
            "spliced    500\n",
            "ai         500\n",
            "Name: count, dtype: int64\n",
            "--------------------\n",
            "Combined Test Set: 1500 images\n",
            "label\n",
            "real       500\n",
            "spliced    500\n",
            "ai         500\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path    label\n",
              "0  /root/.cache/kagglehub/datasets/erentahir/dis2...  spliced\n",
              "1  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...       ai\n",
              "2  /root/.cache/kagglehub/datasets/erentahir/dis2...  spliced\n",
              "3  /root/.cache/kagglehub/datasets/gpch2159/ai-vs...       ai\n",
              "4  /root/.cache/kagglehub/datasets/tristanzhang32...     real"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f263f4f-0158-4e01-b706-1e401454ca27\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "      <td>spliced</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "      <td>ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/erentahir/dis2...</td>\n",
              "      <td>spliced</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/gpch2159/ai-vs...</td>\n",
              "      <td>ai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/root/.cache/kagglehub/datasets/tristanzhang32...</td>\n",
              "      <td>real</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f263f4f-0158-4e01-b706-1e401454ca27')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4f263f4f-0158-4e01-b706-1e401454ca27 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4f263f4f-0158-4e01-b706-1e401454ca27');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(train_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"/root/.cache/kagglehub/datasets/gpch2159/ai-vs-human-syn-imgs-v2-partial/versions/9/stabilityai.stable-diffusion-xl-refiner-1.0_0.5_12_2025.02.25_05.15.08_846327/syn_0.5_12_137871692f794b9bb881294fa00edbc2.jpg\",\n          \"/root/.cache/kagglehub/datasets/tristanzhang32/ai-generated-images-vs-real-images/versions/2/train/real/0057.jpg\",\n          \"/root/.cache/kagglehub/datasets/erentahir/dis25k/versions/1/DIS25k/images/11446.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"spliced\",\n          \"ai\",\n          \"real\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Training transform: minimal transformations (Resize + Normalize only)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transform: no augmentation, just resize + normalize\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "DlqIG-BtPWAf"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        # Define label mapping\n",
        "        self.label_map = {'real': 0, 'ai': 1, 'spliced': 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Use iloc to access the row by integer index\n",
        "        row = self.df.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        label_str = row['label']\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image at {image_path}: {e}\")\n",
        "            # Return a black image or handle appropriately.\n",
        "            # Here we'll generate a dummy image to prevent crashing,\n",
        "            # but in production you might want to skip or raise.\n",
        "            image = Image.new('RGB', (224, 224))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.label_map.get(label_str, -1)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "jaoPzzhIUSkA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_df, transform=train_transform)\n",
        "val_dataset = ImageDataset(val_df, transform=val_transform)\n",
        "test_dataset = ImageDataset(test_df, transform=val_transform)"
      ],
      "metadata": {
        "id": "-yzvVLb8UkxL"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA for acceleration\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS for acceleration\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "#num_classes = len(label_encoder.classes_)\n",
        "\n",
        "resnet50.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(resnet50.fc.in_features, 3)\n",
        ")\n",
        "\n",
        "resnet50 = resnet50.to(device)\n",
        "\n",
        "# Weighted Loss as requested\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.5, 0.5, 1.5]).to(device))\n",
        "\n",
        "# Differential Learning Rates\n",
        "optimizer = optim.Adam([\n",
        "    {'params': resnet50.layer4.parameters(), 'lr': 1e-5},\n",
        "    {'params': resnet50.fc.parameters(), 'lr': 1e-3}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"Model is ready to train!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlmmreS8VpG6",
        "outputId": "de507951-35c5-4f1e-d9db-a5bc1d4eaf20"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA for acceleration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is ready to train!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "num_epochs = 3\n",
        "best_val_loss = float('inf')\n",
        "patience = 7\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Training Phase with Progress Bar\n",
        "    resnet50.train()\n",
        "    train_running_loss, correct, total = 0.0, 0, 0\n",
        "    train_progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for inputs, labels in train_progress_bar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with autocast():\n",
        "            outputs = resnet50(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Mixed precision backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Metrics\n",
        "        train_running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
        "\n",
        "    train_loss = train_running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation Phase with Progress Bar\n",
        "    resnet50.eval()\n",
        "    val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    val_progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = resnet50(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * val_correct / val_total)\n",
        "\n",
        "    val_loss = val_running_loss / len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model based on validation loss + early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(resnet50.state_dict(), \"best_resnet50_model.pth\")\n",
        "        print(f\"Saved best model! (val_loss: {val_loss:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
        "            break\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcVXj1178pBC",
        "outputId": "a2c31583-10db-495a-826c-d05446de5cd3"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1840623761.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/375 [00:00<?, ?it/s]/tmp/ipython-input-1840623761.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:  62%|██████▏   | 234/375 [02:59<02:05,  1.12it/s, accuracy=78.6, loss=0.129]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (98058240 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3450 | Train Accuracy: 82.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/tmp/ipython-input-1840623761.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.2340 | Validation Accuracy: 92.47%\n",
            "Saved best model! (val_loss: 0.2340)\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1723 | Train Accuracy: 92.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1121 | Validation Accuracy: 94.07%\n",
            "Saved best model! (val_loss: 0.1121)\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0915 | Train Accuracy: 95.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.1040 | Validation Accuracy: 94.80%\n",
            "Saved best model! (val_loss: 0.1040)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def test_model(model, test_loader, device, model_path):\n",
        "\n",
        "    print(\"Loading the best model for testing...\")\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_running_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_progress_bar:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Collect predictions and labels for confusion matrix\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            test_progress_bar.set_postfix(loss=loss.item(), accuracy=100 * test_correct / test_total)\n",
        "\n",
        "    test_loss = test_running_loss / len(test_loader)\n",
        "    test_acc = 100 * test_correct / test_total\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Generate Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['real', 'ai', 'spliced'],\n",
        "                yticklabels=['real', 'ai', 'spliced'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "test_loss, test_acc = test_model(resnet50, test_loader, device, \"best_resnet50_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "TIUGP-1J-eIU",
        "outputId": "2c53a1c9-90b9-42ab-9110-9fe87c8c89cc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the best model for testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.1066 | Test Accuracy: 96.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d7fec5",
        "outputId": "d04a8779-f605-4479-997d-f0e333e0872a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# Re-instantiate the model structure\n",
        "resnet50_inference = models.resnet50(pretrained=False)\n",
        "resnet50_inference.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(resnet50_inference.fc.in_features, 3)\n",
        ")\n",
        "\n",
        "# Load the saved state dictionary\n",
        "resnet50_inference.load_state_dict(torch.load(\"best_resnet50_model.pth\"))\n",
        "resnet50_inference.eval() # Set model to evaluation mode\n",
        "\n",
        "# Move model to the correct device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "resnet50_inference = resnet50_inference.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully on {device}!\")\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4eda98",
        "outputId": "e06dc7fd-4942-47c2-c34c-d197285299c1"
      },
      "source": [
        "# Define the transformations for a single image (similar to ImageTransform, but without augmentation)\n",
        "inference_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/Screenshot_20260214-144506_1.png\"\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Apply transformations\n",
        "input_tensor = inference_transform(image)\n",
        "input_batch = input_tensor.unsqueeze(0) # Create a mini-batch as expected by the model\n",
        "\n",
        "# Move to the same device as the model\n",
        "input_batch = input_batch.to(device)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    output = resnet50_inference(input_batch)\n",
        "\n",
        "# Get the predicted class\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "predicted_class_idx = torch.argmax(probabilities).item()\n",
        "\n",
        "# Reverse the label mapping to get the class name\n",
        "label_map = {'real': 0, 'ai': 1, 'spliced': 2}\n",
        "reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "predicted_label = reverse_label_map[predicted_class_idx]\n",
        "\n",
        "print(f\"Predicted class for {image_path}: {predicted_label} (Probability: {probabilities[predicted_class_idx].item():.4f})\")\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for /content/Screenshot_20260214-144506_1.png: real (Probability: 0.5596)\n"
          ]
        }
      ]
    }
  ]
}